2022-07-31 01:49:09 - INFO - Start
2022-07-31 01:49:09 - INFO - ==================================================================
2022-07-31 01:49:09 - INFO - Config - pretrain:
# Pretrain file num
PRETRAIN_FILE_NUM = 15
LOAD_DATA_TYPE = 'mem'#'fluid'
# Training params
NUM_FOLDS = 1
SEED = 42
# BATCH_SIZE = 128
BATCH_SIZE = 44
NUM_EPOCHS = 5
WARMUP_RATIO = 0.15
REINIT_LAYER = 0
WEIGHT_DECAY = 0.01
LR = {'others':6e-5, 'roberta':6e-5, 'newfc_videoreg':6e-5}
LR_LAYER_DECAY = 1.0
# PRETRAIN_TASK = ['tag', 'mlm', 'mfm']
# mlm
PRETRAIN_TASK = ['mlm','itm','mfm','itc']
2022-07-31 01:49:09 - INFO - ==================================================================
2022-07-31 01:49:09 - INFO - Config - model:
MODEL_TYPE = 'uni'#'all', 'cross', 'frame', 'bi', 'uni'

MODEL_CONFIG = {
    'INPUT_SIZE': 1792,
    'HIDDEN_SIZE': 256,
    'NUM_CLASSES': 10000,
    'FEATURE_SIZE': 768,
    'OUTPUT_SIZE': 1024,
    'EXPANSION_SIZE': 2,
    'CLUSTER_SIZE': 64,
    'NUM_GROUPS': 8,
    'DROPOUT_PROB': 0.2,
}

BERT_CFG_DICT = {}
BERT_CFG_DICT['uni'] = {
    'hidden_size':768,
    'num_hidden_layers':6,
    'num_attention_heads':12,
    'intermediate_size':3072,
    'hidden_dropout_prob':0.0,
    'attention_probs_dropout_prob':0.0
}
2022-07-31 01:49:09 - INFO - ==================================================================
2022-07-31 01:49:09 - INFO - Config - data:
DATA_PATH = '/chenqs_ms/VX_race_data/'
BERT_PATH = '/home/tione/notebook/env/baseline/opensource_models/chinese-macbert-base'

DESC = {
    'tag_id':"int",
    'id': 'byte',
    'category_id': 'int',
    'title': 'byte',
    'asr_text': 'byte',
    'frame_feature': 'bytes'
}

DESC_NOTAG = {
    'id': 'byte',
    'title': 'byte',
    'asr_text': 'byte',
    'frame_feature': 'bytes'
}
2022-07-31 01:49:09 - INFO - Model_type = uni
2022-07-31 01:49:09 - INFO - ==================================================================
2022-07-31 01:49:09 - INFO - Load data into memory
2022-07-31 01:49:09 - INFO - Creating dataset
2022-07-31 01:49:32 - INFO - ============================训练集的样本数:1100000==================================
2022-07-31 01:49:33 - INFO - len(train_loader)=25000
2022-07-31 01:49:33 - INFO - Dataset used memory = 2.8GB
2022-07-31 01:49:33 - INFO - Total train steps=125000, warmup steps=18750
2022-07-31 01:49:33 - INFO - Creating model
2022-07-31 01:49:38 - INFO - ============================bert layer:11下面这些参数的学习率是:7e-05
2022-07-31 01:49:38 - INFO - ['text_encoder.bert.encoder.layer.11.attention.self.query.weight', 'text_encoder.bert.encoder.layer.11.attention.self.key.weight', 'text_encoder.bert.encoder.layer.11.attention.self.value.weight', 'text_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.11.output.dense.weight', 'text_encoder_m.bert.encoder.layer.11.attention.self.query.weight', 'text_encoder_m.bert.encoder.layer.11.attention.self.key.weight', 'text_encoder_m.bert.encoder.layer.11.attention.self.value.weight', 'text_encoder_m.bert.encoder.layer.11.attention.output.dense.weight', 'text_encoder_m.bert.encoder.layer.11.intermediate.dense.weight', 'text_encoder_m.bert.encoder.layer.11.output.dense.weight']
2022-07-31 01:49:38 - INFO - ['text_encoder.bert.encoder.layer.11.attention.self.query.bias', 'text_encoder.bert.encoder.layer.11.attention.self.key.bias', 'text_encoder.bert.encoder.layer.11.attention.self.value.bias', 'text_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.11.output.dense.bias', 'text_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.11.attention.self.query.bias', 'text_encoder_m.bert.encoder.layer.11.attention.self.key.bias', 'text_encoder_m.bert.encoder.layer.11.attention.self.value.bias', 'text_encoder_m.bert.encoder.layer.11.attention.output.dense.bias', 'text_encoder_m.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.11.intermediate.dense.bias', 'text_encoder_m.bert.encoder.layer.11.output.dense.bias', 'text_encoder_m.bert.encoder.layer.11.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.11.output.LayerNorm.bias']
2022-07-31 01:49:38 - INFO - ============================bert layer:10下面这些参数的学习率是:6.649999999999999e-05
2022-07-31 01:49:38 - INFO - ['text_encoder.bert.encoder.layer.10.attention.self.query.weight', 'text_encoder.bert.encoder.layer.10.attention.self.key.weight', 'text_encoder.bert.encoder.layer.10.attention.self.value.weight', 'text_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.10.output.dense.weight', 'text_encoder_m.bert.encoder.layer.10.attention.self.query.weight', 'text_encoder_m.bert.encoder.layer.10.attention.self.key.weight', 'text_encoder_m.bert.encoder.layer.10.attention.self.value.weight', 'text_encoder_m.bert.encoder.layer.10.attention.output.dense.weight', 'text_encoder_m.bert.encoder.layer.10.intermediate.dense.weight', 'text_encoder_m.bert.encoder.layer.10.output.dense.weight']
2022-07-31 01:49:38 - INFO - ['text_encoder.bert.encoder.layer.10.attention.self.query.bias', 'text_encoder.bert.encoder.layer.10.attention.self.key.bias', 'text_encoder.bert.encoder.layer.10.attention.self.value.bias', 'text_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.10.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.10.output.dense.bias', 'text_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.10.attention.self.query.bias', 'text_encoder_m.bert.encoder.layer.10.attention.self.key.bias', 'text_encoder_m.bert.encoder.layer.10.attention.self.value.bias', 'text_encoder_m.bert.encoder.layer.10.attention.output.dense.bias', 'text_encoder_m.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.10.intermediate.dense.bias', 'text_encoder_m.bert.encoder.layer.10.output.dense.bias', 'text_encoder_m.bert.encoder.layer.10.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.10.output.LayerNorm.bias']
2022-07-31 01:49:38 - INFO - ============================bert layer:9下面这些参数的学习率是:6.317499999999999e-05
2022-07-31 01:49:38 - INFO - ['text_encoder.bert.encoder.layer.9.attention.self.query.weight', 'text_encoder.bert.encoder.layer.9.attention.self.key.weight', 'text_encoder.bert.encoder.layer.9.attention.self.value.weight', 'text_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.9.output.dense.weight', 'text_encoder_m.bert.encoder.layer.9.attention.self.query.weight', 'text_encoder_m.bert.encoder.layer.9.attention.self.key.weight', 'text_encoder_m.bert.encoder.layer.9.attention.self.value.weight', 'text_encoder_m.bert.encoder.layer.9.attention.output.dense.weight', 'text_encoder_m.bert.encoder.layer.9.intermediate.dense.weight', 'text_encoder_m.bert.encoder.layer.9.output.dense.weight']
2022-07-31 01:49:38 - INFO - ['text_encoder.bert.encoder.layer.9.attention.self.query.bias', 'text_encoder.bert.encoder.layer.9.attention.self.key.bias', 'text_encoder.bert.encoder.layer.9.attention.self.value.bias', 'text_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.9.output.dense.bias', 'text_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.9.attention.self.query.bias', 'text_encoder_m.bert.encoder.layer.9.attention.self.key.bias', 'text_encoder_m.bert.encoder.layer.9.attention.self.value.bias', 'text_encoder_m.bert.encoder.layer.9.attention.output.dense.bias', 'text_encoder_m.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.9.intermediate.dense.bias', 'text_encoder_m.bert.encoder.layer.9.output.dense.bias', 'text_encoder_m.bert.encoder.layer.9.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.9.output.LayerNorm.bias']
2022-07-31 01:49:38 - INFO - ============================bert layer:8下面这些参数的学习率是:6.0016249999999986e-05
2022-07-31 01:49:38 - INFO - ['text_encoder.bert.encoder.layer.8.attention.self.query.weight', 'text_encoder.bert.encoder.layer.8.attention.self.key.weight', 'text_encoder.bert.encoder.layer.8.attention.self.value.weight', 'text_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.8.output.dense.weight', 'text_encoder_m.bert.encoder.layer.8.attention.self.query.weight', 'text_encoder_m.bert.encoder.layer.8.attention.self.key.weight', 'text_encoder_m.bert.encoder.layer.8.attention.self.value.weight', 'text_encoder_m.bert.encoder.layer.8.attention.output.dense.weight', 'text_encoder_m.bert.encoder.layer.8.intermediate.dense.weight', 'text_encoder_m.bert.encoder.layer.8.output.dense.weight']
2022-07-31 01:49:38 - INFO - ['text_encoder.bert.encoder.layer.8.attention.self.query.bias', 'text_encoder.bert.encoder.layer.8.attention.self.key.bias', 'text_encoder.bert.encoder.layer.8.attention.self.value.bias', 'text_encoder.bert.encoder.layer.8.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.8.output.dense.bias', 'text_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.8.attention.self.query.bias', 'text_encoder_m.bert.encoder.layer.8.attention.self.key.bias', 'text_encoder_m.bert.encoder.layer.8.attention.self.value.bias', 'text_encoder_m.bert.encoder.layer.8.attention.output.dense.bias', 'text_encoder_m.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.8.intermediate.dense.bias', 'text_encoder_m.bert.encoder.layer.8.output.dense.bias', 'text_encoder_m.bert.encoder.layer.8.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.8.output.LayerNorm.bias']
2022-07-31 01:49:38 - INFO - ============================bert layer:7下面这些参数的学习率是:5.701543749999999e-05
2022-07-31 01:49:38 - INFO - ['text_encoder.bert.encoder.layer.7.attention.self.query.weight', 'text_encoder.bert.encoder.layer.7.attention.self.key.weight', 'text_encoder.bert.encoder.layer.7.attention.self.value.weight', 'text_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.7.output.dense.weight', 'text_encoder_m.bert.encoder.layer.7.attention.self.query.weight', 'text_encoder_m.bert.encoder.layer.7.attention.self.key.weight', 'text_encoder_m.bert.encoder.layer.7.attention.self.value.weight', 'text_encoder_m.bert.encoder.layer.7.attention.output.dense.weight', 'text_encoder_m.bert.encoder.layer.7.intermediate.dense.weight', 'text_encoder_m.bert.encoder.layer.7.output.dense.weight']
2022-07-31 01:49:38 - INFO - ['text_encoder.bert.encoder.layer.7.attention.self.query.bias', 'text_encoder.bert.encoder.layer.7.attention.self.key.bias', 'text_encoder.bert.encoder.layer.7.attention.self.value.bias', 'text_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.7.output.dense.bias', 'text_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.7.attention.self.query.bias', 'text_encoder_m.bert.encoder.layer.7.attention.self.key.bias', 'text_encoder_m.bert.encoder.layer.7.attention.self.value.bias', 'text_encoder_m.bert.encoder.layer.7.attention.output.dense.bias', 'text_encoder_m.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.7.intermediate.dense.bias', 'text_encoder_m.bert.encoder.layer.7.output.dense.bias', 'text_encoder_m.bert.encoder.layer.7.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.7.output.LayerNorm.bias']
2022-07-31 01:49:38 - INFO - ============================bert layer:6下面这些参数的学习率是:5.416466562499999e-05
2022-07-31 01:49:38 - INFO - ['text_encoder.bert.encoder.layer.6.attention.self.query.weight', 'text_encoder.bert.encoder.layer.6.attention.self.key.weight', 'text_encoder.bert.encoder.layer.6.attention.self.value.weight', 'text_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.6.output.dense.weight', 'text_encoder_m.bert.encoder.layer.6.attention.self.query.weight', 'text_encoder_m.bert.encoder.layer.6.attention.self.key.weight', 'text_encoder_m.bert.encoder.layer.6.attention.self.value.weight', 'text_encoder_m.bert.encoder.layer.6.attention.output.dense.weight', 'text_encoder_m.bert.encoder.layer.6.intermediate.dense.weight', 'text_encoder_m.bert.encoder.layer.6.output.dense.weight']
2022-07-31 01:49:38 - INFO - ['text_encoder.bert.encoder.layer.6.attention.self.query.bias', 'text_encoder.bert.encoder.layer.6.attention.self.key.bias', 'text_encoder.bert.encoder.layer.6.attention.self.value.bias', 'text_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.6.output.dense.bias', 'text_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.6.attention.self.query.bias', 'text_encoder_m.bert.encoder.layer.6.attention.self.key.bias', 'text_encoder_m.bert.encoder.layer.6.attention.self.value.bias', 'text_encoder_m.bert.encoder.layer.6.attention.output.dense.bias', 'text_encoder_m.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.6.intermediate.dense.bias', 'text_encoder_m.bert.encoder.layer.6.output.dense.bias', 'text_encoder_m.bert.encoder.layer.6.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.6.output.LayerNorm.bias']
2022-07-31 01:49:38 - INFO - ============================bert layer:5下面这些参数的学习率是:5.1456432343749983e-05
2022-07-31 01:49:38 - INFO - ['text_encoder.bert.encoder.layer.5.attention.self.query.weight', 'text_encoder.bert.encoder.layer.5.attention.self.key.weight', 'text_encoder.bert.encoder.layer.5.attention.self.value.weight', 'text_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.5.output.dense.weight', 'text_encoder_m.bert.encoder.layer.5.attention.self.query.weight', 'text_encoder_m.bert.encoder.layer.5.attention.self.key.weight', 'text_encoder_m.bert.encoder.layer.5.attention.self.value.weight', 'text_encoder_m.bert.encoder.layer.5.attention.output.dense.weight', 'text_encoder_m.bert.encoder.layer.5.intermediate.dense.weight', 'text_encoder_m.bert.encoder.layer.5.output.dense.weight']
2022-07-31 01:49:38 - INFO - ['text_encoder.bert.encoder.layer.5.attention.self.query.bias', 'text_encoder.bert.encoder.layer.5.attention.self.key.bias', 'text_encoder.bert.encoder.layer.5.attention.self.value.bias', 'text_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.5.output.dense.bias', 'text_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.5.attention.self.query.bias', 'text_encoder_m.bert.encoder.layer.5.attention.self.key.bias', 'text_encoder_m.bert.encoder.layer.5.attention.self.value.bias', 'text_encoder_m.bert.encoder.layer.5.attention.output.dense.bias', 'text_encoder_m.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.5.intermediate.dense.bias', 'text_encoder_m.bert.encoder.layer.5.output.dense.bias', 'text_encoder_m.bert.encoder.layer.5.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.5.output.LayerNorm.bias']
2022-07-31 01:49:39 - INFO - ============================bert layer:4下面这些参数的学习率是:4.8883610726562475e-05
2022-07-31 01:49:39 - INFO - ['text_encoder.bert.encoder.layer.4.attention.self.query.weight', 'text_encoder.bert.encoder.layer.4.attention.self.key.weight', 'text_encoder.bert.encoder.layer.4.attention.self.value.weight', 'text_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.4.output.dense.weight', 'text_encoder_m.bert.encoder.layer.4.attention.self.query.weight', 'text_encoder_m.bert.encoder.layer.4.attention.self.key.weight', 'text_encoder_m.bert.encoder.layer.4.attention.self.value.weight', 'text_encoder_m.bert.encoder.layer.4.attention.output.dense.weight', 'text_encoder_m.bert.encoder.layer.4.intermediate.dense.weight', 'text_encoder_m.bert.encoder.layer.4.output.dense.weight']
2022-07-31 01:49:39 - INFO - ['text_encoder.bert.encoder.layer.4.attention.self.query.bias', 'text_encoder.bert.encoder.layer.4.attention.self.key.bias', 'text_encoder.bert.encoder.layer.4.attention.self.value.bias', 'text_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.4.output.dense.bias', 'text_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.4.attention.self.query.bias', 'text_encoder_m.bert.encoder.layer.4.attention.self.key.bias', 'text_encoder_m.bert.encoder.layer.4.attention.self.value.bias', 'text_encoder_m.bert.encoder.layer.4.attention.output.dense.bias', 'text_encoder_m.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.4.intermediate.dense.bias', 'text_encoder_m.bert.encoder.layer.4.output.dense.bias', 'text_encoder_m.bert.encoder.layer.4.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.4.output.LayerNorm.bias']
2022-07-31 01:49:39 - INFO - ============================bert layer:3下面这些参数的学习率是:4.6439430190234354e-05
2022-07-31 01:49:39 - INFO - ['text_encoder.bert.encoder.layer.3.attention.self.query.weight', 'text_encoder.bert.encoder.layer.3.attention.self.key.weight', 'text_encoder.bert.encoder.layer.3.attention.self.value.weight', 'text_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.3.output.dense.weight', 'text_encoder_m.bert.encoder.layer.3.attention.self.query.weight', 'text_encoder_m.bert.encoder.layer.3.attention.self.key.weight', 'text_encoder_m.bert.encoder.layer.3.attention.self.value.weight', 'text_encoder_m.bert.encoder.layer.3.attention.output.dense.weight', 'text_encoder_m.bert.encoder.layer.3.intermediate.dense.weight', 'text_encoder_m.bert.encoder.layer.3.output.dense.weight']
2022-07-31 01:49:39 - INFO - ['text_encoder.bert.encoder.layer.3.attention.self.query.bias', 'text_encoder.bert.encoder.layer.3.attention.self.key.bias', 'text_encoder.bert.encoder.layer.3.attention.self.value.bias', 'text_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.3.output.dense.bias', 'text_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.3.attention.self.query.bias', 'text_encoder_m.bert.encoder.layer.3.attention.self.key.bias', 'text_encoder_m.bert.encoder.layer.3.attention.self.value.bias', 'text_encoder_m.bert.encoder.layer.3.attention.output.dense.bias', 'text_encoder_m.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.3.intermediate.dense.bias', 'text_encoder_m.bert.encoder.layer.3.output.dense.bias', 'text_encoder_m.bert.encoder.layer.3.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.3.output.LayerNorm.bias']
2022-07-31 01:49:40 - INFO - ============================bert layer:2下面这些参数的学习率是:4.411745868072263e-05
2022-07-31 01:49:40 - INFO - ['text_encoder.bert.encoder.layer.2.attention.self.query.weight', 'text_encoder.bert.encoder.layer.2.attention.self.key.weight', 'text_encoder.bert.encoder.layer.2.attention.self.value.weight', 'text_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.2.output.dense.weight', 'text_encoder_m.bert.encoder.layer.2.attention.self.query.weight', 'text_encoder_m.bert.encoder.layer.2.attention.self.key.weight', 'text_encoder_m.bert.encoder.layer.2.attention.self.value.weight', 'text_encoder_m.bert.encoder.layer.2.attention.output.dense.weight', 'text_encoder_m.bert.encoder.layer.2.intermediate.dense.weight', 'text_encoder_m.bert.encoder.layer.2.output.dense.weight']
2022-07-31 01:49:40 - INFO - ['text_encoder.bert.encoder.layer.2.attention.self.query.bias', 'text_encoder.bert.encoder.layer.2.attention.self.key.bias', 'text_encoder.bert.encoder.layer.2.attention.self.value.bias', 'text_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.2.output.dense.bias', 'text_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.2.attention.self.query.bias', 'text_encoder_m.bert.encoder.layer.2.attention.self.key.bias', 'text_encoder_m.bert.encoder.layer.2.attention.self.value.bias', 'text_encoder_m.bert.encoder.layer.2.attention.output.dense.bias', 'text_encoder_m.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.2.intermediate.dense.bias', 'text_encoder_m.bert.encoder.layer.2.output.dense.bias', 'text_encoder_m.bert.encoder.layer.2.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.2.output.LayerNorm.bias']
2022-07-31 01:49:40 - INFO - ============================bert layer:1下面这些参数的学习率是:4.19115857466865e-05
2022-07-31 01:49:40 - INFO - ['text_encoder.bert.encoder.layer.1.attention.self.query.weight', 'text_encoder.bert.encoder.layer.1.attention.self.key.weight', 'text_encoder.bert.encoder.layer.1.attention.self.value.weight', 'text_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.1.output.dense.weight', 'text_encoder_m.bert.encoder.layer.1.attention.self.query.weight', 'text_encoder_m.bert.encoder.layer.1.attention.self.key.weight', 'text_encoder_m.bert.encoder.layer.1.attention.self.value.weight', 'text_encoder_m.bert.encoder.layer.1.attention.output.dense.weight', 'text_encoder_m.bert.encoder.layer.1.intermediate.dense.weight', 'text_encoder_m.bert.encoder.layer.1.output.dense.weight']
2022-07-31 01:49:40 - INFO - ['text_encoder.bert.encoder.layer.1.attention.self.query.bias', 'text_encoder.bert.encoder.layer.1.attention.self.key.bias', 'text_encoder.bert.encoder.layer.1.attention.self.value.bias', 'text_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.1.output.dense.bias', 'text_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.1.attention.self.query.bias', 'text_encoder_m.bert.encoder.layer.1.attention.self.key.bias', 'text_encoder_m.bert.encoder.layer.1.attention.self.value.bias', 'text_encoder_m.bert.encoder.layer.1.attention.output.dense.bias', 'text_encoder_m.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.1.intermediate.dense.bias', 'text_encoder_m.bert.encoder.layer.1.output.dense.bias', 'text_encoder_m.bert.encoder.layer.1.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.1.output.LayerNorm.bias']
2022-07-31 01:49:40 - INFO - ============================bert layer:0下面这些参数的学习率是:3.9816006459352164e-05
2022-07-31 01:49:40 - INFO - ['text_encoder.bert.encoder.layer.0.attention.self.query.weight', 'text_encoder.bert.encoder.layer.0.attention.self.key.weight', 'text_encoder.bert.encoder.layer.0.attention.self.value.weight', 'text_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.0.output.dense.weight', 'text_encoder_m.bert.encoder.layer.0.attention.self.query.weight', 'text_encoder_m.bert.encoder.layer.0.attention.self.key.weight', 'text_encoder_m.bert.encoder.layer.0.attention.self.value.weight', 'text_encoder_m.bert.encoder.layer.0.attention.output.dense.weight', 'text_encoder_m.bert.encoder.layer.0.intermediate.dense.weight', 'text_encoder_m.bert.encoder.layer.0.output.dense.weight']
2022-07-31 01:49:40 - INFO - ['text_encoder.bert.encoder.layer.0.attention.self.query.bias', 'text_encoder.bert.encoder.layer.0.attention.self.key.bias', 'text_encoder.bert.encoder.layer.0.attention.self.value.bias', 'text_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.0.output.dense.bias', 'text_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.0.attention.self.query.bias', 'text_encoder_m.bert.encoder.layer.0.attention.self.key.bias', 'text_encoder_m.bert.encoder.layer.0.attention.self.value.bias', 'text_encoder_m.bert.encoder.layer.0.attention.output.dense.bias', 'text_encoder_m.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder_m.bert.encoder.layer.0.intermediate.dense.bias', 'text_encoder_m.bert.encoder.layer.0.output.dense.bias', 'text_encoder_m.bert.encoder.layer.0.output.LayerNorm.weight', 'text_encoder_m.bert.encoder.layer.0.output.LayerNorm.bias']
2022-07-31 01:49:40 - INFO - 
    *************************************************************************************************************
    *************************************************************************************************************
    *************************************************************************************************************
    *************************************************************************************************************
    
2022-07-31 01:49:40 - INFO - ============================bert embedding下面这些参数的学习率是:3.782520613638456e-05
2022-07-31 01:49:40 - INFO - ['text_encoder.bert.embeddings.word_embeddings.weight', 'text_encoder.bert.embeddings.position_embeddings.weight', 'text_encoder.bert.embeddings.token_type_embeddings.weight', 'video_position_embeddings.position_embeddings.weight', 'text_encoder_m.bert.embeddings.word_embeddings.weight', 'text_encoder_m.bert.embeddings.position_embeddings.weight', 'text_encoder_m.bert.embeddings.token_type_embeddings.weight']
2022-07-31 01:49:40 - INFO - ['text_encoder.bert.embeddings.LayerNorm.weight', 'text_encoder.bert.embeddings.LayerNorm.bias', 'video_position_embeddings.LayerNorm.weight', 'video_position_embeddings.LayerNorm.bias', 'text_encoder_m.bert.embeddings.LayerNorm.weight', 'text_encoder_m.bert.embeddings.LayerNorm.bias']
2022-07-31 01:49:40 - INFO - 
    *************************************************************************************************************
    *************************************************************************************************************
    *************************************************************************************************************
    *************************************************************************************************************
    
2022-07-31 01:49:40 - INFO - ============================itm mfm下面这些参数的学习率是:4e-05
2022-07-31 01:49:40 - INFO - ['text_encoder.cls.predictions.transform.dense.weight', 'roberta_mvm_lm_header.predictions.transform.dense.weight', 'roberta_mvm_lm_header.predictions.decoder.weight', 'itm_head.weight', 'text_encoder_m.cls.predictions.transform.dense.weight']
2022-07-31 01:49:40 - INFO - ['text_encoder.cls.predictions.bias', 'text_encoder.cls.predictions.transform.dense.bias', 'text_encoder.cls.predictions.transform.LayerNorm.weight', 'text_encoder.cls.predictions.transform.LayerNorm.bias', 'roberta_mvm_lm_header.predictions.bias', 'roberta_mvm_lm_header.predictions.transform.dense.bias', 'roberta_mvm_lm_header.predictions.transform.LayerNorm.weight', 'roberta_mvm_lm_header.predictions.transform.LayerNorm.bias', 'itm_head.bias', 'text_encoder_m.cls.predictions.bias', 'text_encoder_m.cls.predictions.transform.dense.bias', 'text_encoder_m.cls.predictions.transform.LayerNorm.weight', 'text_encoder_m.cls.predictions.transform.LayerNorm.bias']
2022-07-31 01:49:40 - INFO - =====================参数不在我们的分组有=====================
2022-07-31 01:49:40 - INFO - {'text_transform_2_m.transform_1.weight', 'temp', 'text_transform_2.transform_1.bias', 'text_transform_2_m.transform_1.bias', 'video_transform_2.transform_1.weight', 'video_transform_2.transform_1.bias', 'text_transform_2.transform_1.weight', 'video_transform_2_m.transform_1.bias', 'video_transform_2_m.transform_1.weight'}
2022-07-31 01:49:40 - INFO - ===================他们的学习率将设为默认的4e-05=========================
2022-07-31 01:49:42 - INFO - total step=125000	 one epoch step=25000
2022-07-31 01:57:12 - INFO - Epoch=1/5 	 step=499 	 total_loss= 26.28,itm_loss=0.7388,mlm_loss= 2.685,itc_loss= 21.93,mfm_loss=11.88545962756048,lr[0]=1.8629333333333332e-06
2022-07-31 02:04:33 - INFO - Epoch=1/5 	 step=999 	 total_loss= 24.04,itm_loss=0.6829,mlm_loss= 2.718,itc_loss= 21.81,mfm_loss=9.737989596060446,lr[0]=3.7296e-06
2022-07-31 02:11:53 - INFO - Epoch=1/5 	 step=1499 	 total_loss= 22.57,itm_loss=0.6581,mlm_loss= 2.695,itc_loss= 21.62,mfm_loss=8.406506828183408,lr[0]=5.596266666666666e-06
2022-07-31 02:19:14 - INFO - Epoch=1/5 	 step=1999 	 total_loss= 21.14,itm_loss=0.6439,mlm_loss= 2.644,itc_loss= 21.36,mfm_loss=7.175060400132718,lr[0]=7.462933333333333e-06
2022-07-31 02:26:33 - INFO - Epoch=1/5 	 step=2499 	 total_loss= 19.85,itm_loss=0.6342,mlm_loss= 2.596,itc_loss= 21.09,mfm_loss=6.076129591336199,lr[0]=9.3296e-06
2022-07-31 02:33:54 - INFO - Epoch=1/5 	 step=2999 	 total_loss= 18.74,itm_loss=0.6273,mlm_loss=  2.55,itc_loss= 20.72,mfm_loss=5.203371971875439,lr[0]=1.1196266666666665e-05
2022-07-31 02:41:14 - INFO - Epoch=1/5 	 step=3499 	 total_loss= 17.83,itm_loss=0.6219,mlm_loss= 2.506,itc_loss= 20.35,mfm_loss=4.522962093112655,lr[0]=1.3062933333333332e-05
2022-07-31 02:48:34 - INFO - Epoch=1/5 	 step=3999 	 total_loss= 17.04,itm_loss=0.6173,mlm_loss= 2.467,itc_loss= 19.94,mfm_loss=3.990838299441834,lr[0]=1.4929599999999998e-05
2022-07-31 02:55:54 - INFO - Epoch=1/5 	 step=4499 	 total_loss= 16.39,itm_loss=0.6138,mlm_loss= 2.433,itc_loss= 19.55,mfm_loss=3.5668389567284353,lr[0]=1.6796266666666667e-05
2022-07-31 03:03:13 - INFO - Epoch=1/5 	 step=4999 	 total_loss= 15.84,itm_loss=0.6107,mlm_loss= 2.399,itc_loss= 19.22,mfm_loss=3.2224745369194197,lr[0]=1.866293333333333e-05
2022-07-31 03:10:33 - INFO - Epoch=1/5 	 step=5499 	 total_loss= 15.34,itm_loss=0.6079,mlm_loss=  2.37,itc_loss= 18.84,mfm_loss=2.9377525073871933,lr[0]=2.0529599999999997e-05
2022-07-31 03:17:53 - INFO - Epoch=1/5 	 step=5999 	 total_loss= 14.88,itm_loss=0.6049,mlm_loss= 2.342,itc_loss= 18.48,mfm_loss=2.698657127245975,lr[0]=2.2396266666666664e-05
2022-07-31 03:25:13 - INFO - Epoch=1/5 	 step=6499 	 total_loss= 14.49,itm_loss=0.6016,mlm_loss= 2.317,itc_loss= 18.16,mfm_loss=2.4956700064145227,lr[0]=2.426293333333333e-05
2022-07-31 03:32:33 - INFO - Epoch=1/5 	 step=6999 	 total_loss= 14.12,itm_loss=0.5983,mlm_loss= 2.295,itc_loss= 17.81,mfm_loss=2.320672528741305,lr[0]=2.6129599999999998e-05
2022-07-31 03:39:53 - INFO - Epoch=1/5 	 step=7499 	 total_loss= 13.78,itm_loss=0.5949,mlm_loss= 2.274,itc_loss= 17.49,mfm_loss=2.1688350077711758,lr[0]=2.7996266666666665e-05
2022-07-31 03:47:13 - INFO - Epoch=1/5 	 step=7999 	 total_loss= 13.47,itm_loss= 0.591,mlm_loss= 2.253,itc_loss= 17.18,mfm_loss=2.0353749184471477,lr[0]=2.9862933333333332e-05
2022-07-31 03:54:32 - INFO - Epoch=1/5 	 step=8499 	 total_loss= 13.18,itm_loss=0.5866,mlm_loss= 2.236,itc_loss= 16.87,mfm_loss=1.9174982704593295,lr[0]=3.1729599999999995e-05
2022-07-31 04:01:52 - INFO - Epoch=1/5 	 step=8999 	 total_loss=  12.9,itm_loss=0.5818,mlm_loss=  2.22,itc_loss= 16.58,mfm_loss=1.812618337122929,lr[0]=3.359626666666666e-05
2022-07-31 04:09:11 - INFO - Epoch=1/5 	 step=9499 	 total_loss= 12.64,itm_loss=0.5762,mlm_loss= 2.204,itc_loss= 16.28,mfm_loss=1.7184633659086925,lr[0]=3.546293333333333e-05
2022-07-31 04:16:31 - INFO - Epoch=1/5 	 step=9999 	 total_loss=  12.4,itm_loss=0.5705,mlm_loss=  2.19,itc_loss= 16.01,mfm_loss=1.6336364002324582,lr[0]=3.7329599999999996e-05
2022-07-31 04:23:51 - INFO - Epoch=1/5 	 step=10499 	 total_loss= 12.17,itm_loss=0.5649,mlm_loss= 2.177,itc_loss= 15.75,mfm_loss=1.5567485059385204,lr[0]=3.919626666666666e-05
2022-07-31 04:31:10 - INFO - Epoch=1/5 	 step=10999 	 total_loss= 11.96,itm_loss=0.5593,mlm_loss= 2.166,itc_loss=  15.5,mfm_loss=1.4867928278736118,lr[0]=4.106293333333333e-05
2022-07-31 04:38:30 - INFO - Epoch=1/5 	 step=11499 	 total_loss= 11.76,itm_loss=0.5537,mlm_loss= 2.153,itc_loss= 15.26,mfm_loss=1.4228775262363715,lr[0]=4.2929599999999996e-05
2022-07-31 04:45:50 - INFO - Epoch=1/5 	 step=11999 	 total_loss= 11.57,itm_loss=0.5487,mlm_loss= 2.142,itc_loss= 15.03,mfm_loss=1.3641952297409305,lr[0]=4.479626666666666e-05
2022-07-31 04:53:10 - INFO - Epoch=1/5 	 step=12499 	 total_loss= 11.39,itm_loss=0.5437,mlm_loss= 2.131,itc_loss=  14.8,mfm_loss=1.310391696022412,lr[0]=4.666293333333332e-05
2022-07-31 05:00:29 - INFO - Epoch=1/5 	 step=12999 	 total_loss= 11.22,itm_loss=0.5387,mlm_loss= 2.122,itc_loss= 14.59,mfm_loss=1.2603811449897022,lr[0]=4.85296e-05
2022-07-31 05:07:49 - INFO - Epoch=1/5 	 step=13499 	 total_loss= 11.05,itm_loss=0.5341,mlm_loss= 2.113,itc_loss= 14.38,mfm_loss=1.2144571587670965,lr[0]=5.039626666666666e-05
2022-07-31 05:15:09 - INFO - Epoch=1/5 	 step=13999 	 total_loss=  10.9,itm_loss=0.5296,mlm_loss= 2.105,itc_loss= 14.18,mfm_loss=1.1715337931680123,lr[0]=5.226293333333333e-05
2022-07-31 05:22:29 - INFO - Epoch=1/5 	 step=14499 	 total_loss= 10.75,itm_loss=0.5254,mlm_loss= 2.097,itc_loss= 13.99,mfm_loss=1.131395691997893,lr[0]=5.412959999999999e-05
2022-07-31 05:29:48 - INFO - Epoch=1/5 	 step=14999 	 total_loss= 10.61,itm_loss=0.5211,mlm_loss=  2.09,itc_loss= 13.81,mfm_loss=1.09408941703573,lr[0]=5.5996266666666664e-05
2022-07-31 05:37:08 - INFO - Epoch=1/5 	 step=15499 	 total_loss= 10.48,itm_loss=0.5172,mlm_loss= 2.083,itc_loss= 13.63,mfm_loss=1.0591282397635864,lr[0]=5.7862933333333325e-05
2022-07-31 05:44:28 - INFO - Epoch=1/5 	 step=15999 	 total_loss= 10.35,itm_loss=0.5134,mlm_loss= 2.077,itc_loss= 13.47,mfm_loss=1.0264314777848491,lr[0]=5.97296e-05
2022-07-31 05:51:48 - INFO - Epoch=1/5 	 step=16499 	 total_loss= 10.23,itm_loss=0.5099,mlm_loss= 2.071,itc_loss=  13.3,mfm_loss=0.9957722965717943,lr[0]=6.159626666666667e-05
2022-07-31 05:59:08 - INFO - Epoch=1/5 	 step=16999 	 total_loss= 10.11,itm_loss=0.5064,mlm_loss= 2.065,itc_loss= 13.14,mfm_loss=0.9668515755591609,lr[0]=6.346293333333333e-05
2022-07-31 06:06:27 - INFO - Epoch=1/5 	 step=17499 	 total_loss= 9.998,itm_loss=0.5032,mlm_loss=  2.06,itc_loss= 12.99,mfm_loss=0.9395060530937093,lr[0]=6.53296e-05
2022-07-31 06:13:47 - INFO - Epoch=1/5 	 step=17999 	 total_loss= 9.891,itm_loss=   0.5,mlm_loss= 2.054,itc_loss= 12.85,mfm_loss=0.9136386469169766,lr[0]=6.719626666666665e-05
2022-07-31 06:21:07 - INFO - Epoch=1/5 	 step=18499 	 total_loss= 9.788,itm_loss= 0.497,mlm_loss= 2.049,itc_loss= 12.71,mfm_loss=0.8893414551495574,lr[0]=6.906293333333333e-05
2022-07-31 06:28:26 - INFO - Epoch=1/5 	 step=18999 	 total_loss= 9.689,itm_loss=0.4942,mlm_loss= 2.044,itc_loss= 12.57,mfm_loss=0.8662207319421957,lr[0]=6.999905141413459e-05
2022-07-31 06:35:46 - INFO - Epoch=1/5 	 step=19499 	 total_loss= 9.595,itm_loss=0.4915,mlm_loss= 2.039,itc_loss= 12.44,mfm_loss=0.8442659890802676,lr[0]=6.999141726306216e-05
2022-07-31 06:43:05 - INFO - Epoch=1/5 	 step=19999 	 total_loss= 9.504,itm_loss=0.4889,mlm_loss= 2.035,itc_loss= 12.31,mfm_loss=0.8233995737338652,lr[0]=6.997613532999491e-05
2022-07-31 06:50:25 - INFO - Epoch=1/5 	 step=20499 	 total_loss= 9.417,itm_loss=0.4864,mlm_loss= 2.031,itc_loss= 12.19,mfm_loss=0.803484067741137,lr[0]=6.995320895497742e-05
2022-07-31 06:57:45 - INFO - Epoch=1/5 	 step=20999 	 total_loss= 9.333,itm_loss=0.4841,mlm_loss= 2.027,itc_loss= 12.07,mfm_loss=0.7844790281213765,lr[0]=6.992264314883611e-05
2022-07-31 07:05:05 - INFO - Epoch=1/5 	 step=21499 	 total_loss= 9.252,itm_loss=0.4818,mlm_loss= 2.024,itc_loss= 11.96,mfm_loss=0.7663868627652644,lr[0]=6.988444459208408e-05
2022-07-31 07:12:24 - INFO - Epoch=1/5 	 step=21999 	 total_loss= 9.175,itm_loss=0.4794,mlm_loss=  2.02,itc_loss= 11.85,mfm_loss=0.749100845982376,lr[0]=6.983862163346105e-05
2022-07-31 07:19:44 - INFO - Epoch=1/5 	 step=22499 	 total_loss=   9.1,itm_loss=0.4772,mlm_loss= 2.016,itc_loss= 11.75,mfm_loss=0.7325342083946301,lr[0]=6.978518428810857e-05
2022-07-31 07:27:04 - INFO - Epoch=1/5 	 step=22999 	 total_loss= 9.028,itm_loss=0.4751,mlm_loss= 2.013,itc_loss= 11.65,mfm_loss=0.7167228361357308,lr[0]=6.972414423538119e-05
2022-07-31 07:34:23 - INFO - Epoch=1/5 	 step=23499 	 total_loss= 8.959,itm_loss= 0.473,mlm_loss=  2.01,itc_loss= 11.55,mfm_loss=0.7015544100344975,lr[0]=6.965551481629372e-05
2022-07-31 07:41:43 - INFO - Epoch=1/5 	 step=23999 	 total_loss= 8.893,itm_loss= 0.471,mlm_loss= 2.007,itc_loss= 11.46,mfm_loss=0.6871249596905633,lr[0]=6.957931103060544e-05
2022-07-31 07:49:03 - INFO - Epoch=1/5 	 step=24499 	 total_loss= 8.828,itm_loss= 0.469,mlm_loss= 2.004,itc_loss= 11.36,mfm_loss=0.6733077894722762,lr[0]=6.94955495335417e-05
2022-07-31 07:56:22 - INFO - Epoch=1/5 	 step=24999 	 total_loss= 8.766,itm_loss=0.4671,mlm_loss= 2.001,itc_loss= 11.28,mfm_loss=0.6599512971846156,lr[0]=6.940424863215375e-05
2022-07-31 08:04:08 - INFO - Epoch=2/5 	 step=25499 	 total_loss=  5.69,itm_loss=0.3657,mlm_loss= 1.867,itc_loss= 6.899,mfm_loss=0.007660148966360356,lr[0]=6.930542828131753e-05
2022-07-31 08:11:28 - INFO - Epoch=2/5 	 step=25999 	 total_loss= 5.676,itm_loss=0.3679,mlm_loss= 1.859,itc_loss= 6.886,mfm_loss=0.0065184696534967765,lr[0]=6.919911007937223e-05
2022-07-31 08:18:48 - INFO - Epoch=2/5 	 step=26499 	 total_loss= 5.669,itm_loss=0.3662,mlm_loss=  1.86,itc_loss= 6.874,mfm_loss=0.0063286822465754445,lr[0]=6.908531726339982e-05
2022-07-31 08:26:08 - INFO - Epoch=2/5 	 step=26999 	 total_loss= 5.656,itm_loss=0.3659,mlm_loss= 1.855,itc_loss= 6.859,mfm_loss=0.005496642741920128,lr[0]=6.896407470414624e-05
2022-07-31 08:33:28 - INFO - Epoch=2/5 	 step=27499 	 total_loss= 5.646,itm_loss=0.3659,mlm_loss= 1.851,itc_loss= 6.849,mfm_loss=0.005049258133355312,lr[0]=6.883540890058566e-05
2022-07-31 08:40:47 - INFO - Epoch=2/5 	 step=27999 	 total_loss= 5.639,itm_loss=0.3656,mlm_loss= 1.848,itc_loss= 6.842,mfm_loss=0.004788813633146112,lr[0]=6.869934797412876e-05
2022-07-31 08:48:08 - INFO - Epoch=2/5 	 step=28499 	 total_loss= 5.631,itm_loss=0.3651,mlm_loss= 1.846,itc_loss= 6.832,mfm_loss=0.004506880098702372,lr[0]=6.855592166247652e-05
2022-07-31 08:55:27 - INFO - Epoch=2/5 	 step=28999 	 total_loss= 5.629,itm_loss=0.3646,mlm_loss= 1.847,itc_loss= 6.825,mfm_loss=0.004626502134328006,lr[0]=6.84051613131207e-05
2022-07-31 09:02:47 - INFO - Epoch=2/5 	 step=29499 	 total_loss= 5.624,itm_loss=0.3641,mlm_loss= 1.846,itc_loss= 6.819,mfm_loss=0.004500521006358765,lr[0]=6.824709987649245e-05
2022-07-31 09:10:06 - INFO - Epoch=2/5 	 step=29999 	 total_loss= 5.621,itm_loss=0.3638,mlm_loss= 1.848,itc_loss=  6.81,mfm_loss=0.004379446931696339,lr[0]=6.808177189876062e-05
2022-07-31 09:17:26 - INFO - Epoch=2/5 	 step=30499 	 total_loss= 5.615,itm_loss=0.3635,mlm_loss= 1.847,itc_loss= 6.801,mfm_loss=0.0041993053604780185,lr[0]=6.790921351428129e-05
2022-07-31 09:24:46 - INFO - Epoch=2/5 	 step=30999 	 total_loss=  5.61,itm_loss= 0.363,mlm_loss= 1.845,itc_loss= 6.795,mfm_loss=0.004023244066135045,lr[0]=6.77294624377002e-05
2022-07-31 09:32:05 - INFO - Epoch=2/5 	 step=31499 	 total_loss= 5.605,itm_loss=0.3625,mlm_loss= 1.846,itc_loss= 6.787,mfm_loss=0.004035814228414875,lr[0]=6.754255795570975e-05
2022-07-31 09:39:25 - INFO - Epoch=2/5 	 step=31999 	 total_loss= 5.601,itm_loss= 0.362,mlm_loss= 1.845,itc_loss= 6.781,mfm_loss=0.004075703420104474,lr[0]=6.734854091846248e-05
2022-07-31 09:46:45 - INFO - Epoch=2/5 	 step=32499 	 total_loss= 5.597,itm_loss=0.3616,mlm_loss= 1.844,itc_loss= 6.775,mfm_loss=0.00394280299198673,lr[0]=6.714745373064274e-05
2022-07-31 09:54:05 - INFO - Epoch=2/5 	 step=32999 	 total_loss= 5.592,itm_loss=0.3612,mlm_loss= 1.843,itc_loss= 6.768,mfm_loss=0.0038360976378486638,lr[0]=6.693934034219867e-05
2022-07-31 10:01:25 - INFO - Epoch=2/5 	 step=33499 	 total_loss= 5.588,itm_loss=0.3606,mlm_loss= 1.842,itc_loss= 6.763,mfm_loss=0.0037753752773171463,lr[0]=6.672424623873644e-05
2022-07-31 10:08:45 - INFO - Epoch=2/5 	 step=33999 	 total_loss= 5.583,itm_loss=0.3601,mlm_loss= 1.842,itc_loss= 6.755,mfm_loss=0.003720469004440017,lr[0]=6.650221843157884e-05
2022-07-31 10:16:05 - INFO - Epoch=2/5 	 step=34499 	 total_loss= 5.578,itm_loss=0.3596,mlm_loss=  1.84,itc_loss= 6.748,mfm_loss=0.003620254327418258,lr[0]=6.627330544749039e-05
2022-07-31 10:23:24 - INFO - Epoch=2/5 	 step=34999 	 total_loss= 5.573,itm_loss=0.3588,mlm_loss= 1.839,itc_loss= 6.743,mfm_loss=0.003548152630792304,lr[0]=6.603755731807128e-05
2022-07-31 10:30:45 - INFO - Epoch=2/5 	 step=35499 	 total_loss= 5.569,itm_loss=0.3583,mlm_loss= 1.838,itc_loss= 6.738,mfm_loss=0.0035739055619710565,lr[0]=6.579502556882233e-05
2022-07-31 10:38:05 - INFO - Epoch=2/5 	 step=35999 	 total_loss= 5.566,itm_loss=0.3578,mlm_loss= 1.838,itc_loss= 6.733,mfm_loss=0.0035624633476797787,lr[0]=6.554576320788357e-05
2022-07-31 10:45:25 - INFO - Epoch=2/5 	 step=36499 	 total_loss= 5.562,itm_loss=0.3575,mlm_loss= 1.838,itc_loss= 6.728,mfm_loss=0.0035060021457608043,lr[0]=6.528982471444858e-05
2022-07-31 10:52:44 - INFO - Epoch=2/5 	 step=36999 	 total_loss= 5.559,itm_loss=0.3569,mlm_loss= 1.837,itc_loss= 6.723,mfm_loss=0.003472348139736504,lr[0]=6.502726602685752e-05
2022-07-31 11:00:04 - INFO - Epoch=2/5 	 step=37499 	 total_loss= 5.555,itm_loss=0.3566,mlm_loss= 1.836,itc_loss= 6.718,mfm_loss=0.003463443258231066,lr[0]=6.475814453037118e-05
2022-07-31 11:07:24 - INFO - Epoch=2/5 	 step=37999 	 total_loss= 5.553,itm_loss=0.3562,mlm_loss= 1.836,itc_loss= 6.714,mfm_loss=0.00366982910515495,lr[0]=6.448251904462866e-05
2022-07-31 11:14:44 - INFO - Epoch=2/5 	 step=38499 	 total_loss= 5.549,itm_loss=0.3557,mlm_loss= 1.835,itc_loss= 6.708,mfm_loss=0.0036158420117772747,lr[0]=6.420044981079167e-05
2022-07-31 11:22:04 - INFO - Epoch=2/5 	 step=38999 	 total_loss= 5.546,itm_loss=0.3552,mlm_loss= 1.835,itc_loss= 6.704,mfm_loss=0.0036041533656138007,lr[0]=6.391199847837823e-05
2022-07-31 11:29:24 - INFO - Epoch=2/5 	 step=39499 	 total_loss= 5.542,itm_loss=0.3547,mlm_loss= 1.834,itc_loss= 6.699,mfm_loss=0.003554796805948841,lr[0]=6.361722809178833e-05
2022-07-31 11:36:44 - INFO - Epoch=2/5 	 step=39999 	 total_loss= 5.539,itm_loss=0.3546,mlm_loss= 1.833,itc_loss= 6.695,mfm_loss=0.003652586552238205,lr[0]=6.331620307652489e-05
2022-07-31 11:44:04 - INFO - Epoch=2/5 	 step=40499 	 total_loss= 5.536,itm_loss=0.3542,mlm_loss= 1.833,itc_loss= 6.691,mfm_loss=0.003605776024566672,lr[0]=6.300898922511281e-05
2022-07-31 11:51:24 - INFO - Epoch=2/5 	 step=40999 	 total_loss= 5.534,itm_loss=0.3537,mlm_loss= 1.833,itc_loss= 6.687,mfm_loss=0.003587284116307552,lr[0]=6.269565368271925e-05
2022-07-31 11:58:44 - INFO - Epoch=2/5 	 step=41499 	 total_loss=  5.53,itm_loss=0.3533,mlm_loss= 1.832,itc_loss= 6.683,mfm_loss=0.0035695610317832073,lr[0]=6.237626493247829e-05
2022-07-31 12:06:04 - INFO - Epoch=2/5 	 step=41999 	 total_loss= 5.528,itm_loss=0.3528,mlm_loss= 1.832,itc_loss= 6.679,mfm_loss=0.0035315080628197148,lr[0]=6.205089278052308e-05
2022-07-31 12:13:24 - INFO - Epoch=2/5 	 step=42499 	 total_loss= 5.525,itm_loss=0.3522,mlm_loss= 1.832,itc_loss= 6.675,mfm_loss=0.003480499627872172,lr[0]=6.171960834072898e-05
2022-07-31 12:20:44 - INFO - Epoch=2/5 	 step=42999 	 total_loss= 5.521,itm_loss=0.3519,mlm_loss= 1.831,itc_loss= 6.671,mfm_loss=0.0034040029765894024,lr[0]=6.138248401917073e-05
2022-07-31 12:28:04 - INFO - Epoch=2/5 	 step=43499 	 total_loss= 5.519,itm_loss=0.3514,mlm_loss=  1.83,itc_loss= 6.668,mfm_loss=0.0033673220257183197,lr[0]=6.10395934982973e-05
2022-07-31 12:35:24 - INFO - Epoch=2/5 	 step=43999 	 total_loss= 5.516,itm_loss=0.3509,mlm_loss=  1.83,itc_loss= 6.664,mfm_loss=0.0033473346343116197,lr[0]=6.069101172082767e-05
2022-07-31 12:42:44 - INFO - Epoch=2/5 	 step=44499 	 total_loss= 5.513,itm_loss=0.3504,mlm_loss=  1.83,itc_loss= 6.659,mfm_loss=0.0033129604675936736,lr[0]=6.033681487337134e-05
2022-07-31 12:50:04 - INFO - Epoch=2/5 	 step=44999 	 total_loss=  5.51,itm_loss=0.3499,mlm_loss= 1.829,itc_loss= 6.655,mfm_loss=0.0032614508520274225,lr[0]=5.997708036977671e-05
2022-07-31 12:57:25 - INFO - Epoch=2/5 	 step=45499 	 total_loss= 5.507,itm_loss=0.3495,mlm_loss= 1.828,itc_loss= 6.652,mfm_loss=0.003196645608889827,lr[0]=5.961188683421151e-05
2022-07-31 13:04:45 - INFO - Epoch=2/5 	 step=45999 	 total_loss= 5.504,itm_loss=0.3491,mlm_loss= 1.828,itc_loss= 6.648,mfm_loss=0.003141846762907708,lr[0]=5.9241314083978515e-05
2022-07-31 13:12:05 - INFO - Epoch=2/5 	 step=46499 	 total_loss= 5.501,itm_loss=0.3488,mlm_loss= 1.827,itc_loss= 6.644,mfm_loss=0.003097166267221002,lr[0]=5.8865443112070534e-05
2022-07-31 13:19:24 - INFO - Epoch=2/5 	 step=46999 	 total_loss= 5.499,itm_loss=0.3484,mlm_loss= 1.827,itc_loss= 6.641,mfm_loss=0.003063391305653107,lr[0]=5.8484356069468464e-05
2022-07-31 13:26:44 - INFO - Epoch=2/5 	 step=47499 	 total_loss= 5.496,itm_loss=0.3479,mlm_loss= 1.826,itc_loss= 6.638,mfm_loss=0.0030445339315939113,lr[0]=5.809813624718624e-05
2022-07-31 13:34:04 - INFO - Epoch=2/5 	 step=47999 	 total_loss= 5.494,itm_loss=0.3474,mlm_loss= 1.826,itc_loss= 6.635,mfm_loss=0.0030193362902372805,lr[0]=5.770686805806662e-05
2022-07-31 13:41:24 - INFO - Epoch=2/5 	 step=48499 	 total_loss= 5.492,itm_loss=0.3471,mlm_loss= 1.826,itc_loss= 6.632,mfm_loss=0.0029915110640252747,lr[0]=5.731063701833174e-05
2022-07-31 13:48:44 - INFO - Epoch=2/5 	 step=48999 	 total_loss= 5.489,itm_loss=0.3467,mlm_loss= 1.825,itc_loss= 6.629,mfm_loss=0.0029852351365494524,lr[0]=5.6909529728892573e-05
2022-07-31 13:56:04 - INFO - Epoch=2/5 	 step=49499 	 total_loss= 5.486,itm_loss=0.3463,mlm_loss= 1.824,itc_loss= 6.626,mfm_loss=0.002943463375994692,lr[0]=5.650363385642132e-05
2022-07-31 14:03:24 - INFO - Epoch=2/5 	 step=49999 	 total_loss= 5.483,itm_loss=0.3459,mlm_loss= 1.823,itc_loss= 6.623,mfm_loss=0.002926273954342025,lr[0]=5.609303811419075e-05
2022-07-31 14:11:10 - INFO - Epoch=3/5 	 step=50499 	 total_loss= 5.335,itm_loss=0.3175,mlm_loss= 1.806,itc_loss= 6.415,mfm_loss=0.0034988643955115345,lr[0]=5.567783224268499e-05
2022-07-31 14:18:30 - INFO - Epoch=3/5 	 step=50999 	 total_loss= 5.336,itm_loss=0.3183,mlm_loss=   1.8,itc_loss= 6.431,mfm_loss=0.002479361917383403,lr[0]=5.525810698998559e-05
2022-07-31 14:25:51 - INFO - Epoch=3/5 	 step=51499 	 total_loss= 5.334,itm_loss= 0.318,mlm_loss=   1.8,itc_loss= 6.427,mfm_loss=0.0021653674600749058,lr[0]=5.483395409193757e-05
2022-07-31 14:33:10 - INFO - Epoch=3/5 	 step=51999 	 total_loss= 5.329,itm_loss=0.3168,mlm_loss= 1.796,itc_loss= 6.428,mfm_loss=0.002049853012143269,lr[0]=5.440546625209941e-05
2022-07-31 14:40:30 - INFO - Epoch=3/5 	 step=52499 	 total_loss= 5.332,itm_loss=0.3156,mlm_loss= 1.799,itc_loss= 6.431,mfm_loss=0.0018696187297160985,lr[0]=5.397273712148163e-05
2022-07-31 14:47:50 - INFO - Epoch=3/5 	 step=52999 	 total_loss= 5.327,itm_loss=0.3146,mlm_loss= 1.796,itc_loss=  6.43,mfm_loss=0.001749768550283804,lr[0]=5.35358612780783e-05
2022-07-31 14:55:10 - INFO - Epoch=3/5 	 step=53499 	 total_loss= 5.328,itm_loss=0.3133,mlm_loss= 1.799,itc_loss= 6.429,mfm_loss=0.0015380229202445926,lr[0]=5.309493420619583e-05
2022-07-31 15:02:30 - INFO - Epoch=3/5 	 step=53999 	 total_loss= 5.326,itm_loss=0.3136,mlm_loss= 1.798,itc_loss= 6.426,mfm_loss=0.001539656819385594,lr[0]=5.265005227558392e-05
2022-07-31 15:09:50 - INFO - Epoch=3/5 	 step=54499 	 total_loss= 5.326,itm_loss=0.3134,mlm_loss= 1.799,itc_loss= 6.424,mfm_loss=0.001433346667641366,lr[0]=5.220131272037265e-05
2022-07-31 15:17:10 - INFO - Epoch=3/5 	 step=54999 	 total_loss= 5.323,itm_loss=0.3127,mlm_loss= 1.798,itc_loss= 6.421,mfm_loss=0.0013840171175716048,lr[0]=5.174881361782092e-05
2022-07-31 15:24:30 - INFO - Epoch=3/5 	 step=55499 	 total_loss= 5.322,itm_loss=0.3125,mlm_loss= 1.798,itc_loss=  6.42,mfm_loss=0.0013490397032897897,lr[0]=5.129265386688055e-05
2022-07-31 15:31:50 - INFO - Epoch=3/5 	 step=55999 	 total_loss= 5.318,itm_loss=0.3124,mlm_loss= 1.796,itc_loss= 6.417,mfm_loss=0.001271043502891458,lr[0]=5.0832933166580694e-05
2022-07-31 15:39:10 - INFO - Epoch=3/5 	 step=56499 	 total_loss= 5.314,itm_loss= 0.312,mlm_loss= 1.793,itc_loss= 6.417,mfm_loss=0.0012135468876829586,lr[0]=5.036975199423748e-05
2022-07-31 15:46:30 - INFO - Epoch=3/5 	 step=56999 	 total_loss= 5.312,itm_loss= 0.312,mlm_loss= 1.791,itc_loss= 6.416,mfm_loss=0.00122336807892555,lr[0]=4.990321158349353e-05
2022-07-31 15:53:49 - INFO - Epoch=3/5 	 step=57499 	 total_loss=  5.31,itm_loss=0.3115,mlm_loss=  1.79,itc_loss= 6.415,mfm_loss=0.0011692742685364373,lr[0]=4.943341390219219e-05
2022-07-31 16:01:09 - INFO - Epoch=3/5 	 step=57999 	 total_loss= 5.308,itm_loss=0.3116,mlm_loss= 1.788,itc_loss= 6.414,mfm_loss=0.001155393815833548,lr[0]=4.896046163009118e-05
2022-07-31 16:08:29 - INFO - Epoch=3/5 	 step=58499 	 total_loss= 5.305,itm_loss=0.3116,mlm_loss= 1.787,itc_loss= 6.411,mfm_loss=0.0011461883051238075,lr[0]=4.848445813642088e-05
2022-07-31 16:15:48 - INFO - Epoch=3/5 	 step=58999 	 total_loss= 5.304,itm_loss=0.3115,mlm_loss= 1.786,itc_loss= 6.411,mfm_loss=0.001114882224935573,lr[0]=4.8005507457291633e-05
2022-07-31 16:23:09 - INFO - Epoch=3/5 	 step=59499 	 total_loss= 5.303,itm_loss=0.3113,mlm_loss= 1.786,itc_loss=  6.41,mfm_loss=0.0011013908566083125,lr[0]=4.7523714272955546e-05
2022-07-31 16:30:29 - INFO - Epoch=3/5 	 step=59999 	 total_loss=   5.3,itm_loss=0.3112,mlm_loss= 1.784,itc_loss= 6.408,mfm_loss=0.001079520097196232,lr[0]=4.703918388492734e-05
2022-07-31 16:37:49 - INFO - Epoch=3/5 	 step=60499 	 total_loss= 5.298,itm_loss= 0.311,mlm_loss= 1.783,itc_loss= 6.406,mfm_loss=0.001139458242838933,lr[0]=4.655202219296953e-05
2022-07-31 16:45:10 - INFO - Epoch=3/5 	 step=60999 	 total_loss= 5.296,itm_loss=0.3108,mlm_loss= 1.782,itc_loss= 6.404,mfm_loss=0.0011430779635974696,lr[0]=4.606233567194672e-05
2022-07-31 16:52:31 - INFO - Epoch=3/5 	 step=61499 	 total_loss= 5.293,itm_loss=0.3104,mlm_loss=  1.78,itc_loss= 6.402,mfm_loss=0.0011144120063294846,lr[0]=4.557023134855438e-05
2022-07-31 16:59:51 - INFO - Epoch=3/5 	 step=61999 	 total_loss= 5.292,itm_loss=0.3103,mlm_loss=  1.78,itc_loss= 6.401,mfm_loss=0.0010860632662600756,lr[0]=4.5075816777926805e-05
2022-07-31 17:07:12 - INFO - Epoch=3/5 	 step=62499 	 total_loss=  5.29,itm_loss=  0.31,mlm_loss= 1.779,itc_loss=   6.4,mfm_loss=0.0011261325573389248,lr[0]=4.4579200020129814e-05
2022-07-31 17:14:32 - INFO - Epoch=3/5 	 step=62999 	 total_loss= 5.289,itm_loss=0.3096,mlm_loss= 1.778,itc_loss=   6.4,mfm_loss=0.0011369655787448086,lr[0]=4.408048961654282e-05
2022-07-31 17:21:52 - INFO - Epoch=3/5 	 step=63499 	 total_loss= 5.286,itm_loss=0.3091,mlm_loss= 1.777,itc_loss= 6.399,mfm_loss=0.0011234335459792478,lr[0]=4.357979456613598e-05
2022-07-31 17:29:12 - INFO - Epoch=3/5 	 step=63999 	 total_loss= 5.285,itm_loss=0.3088,mlm_loss= 1.776,itc_loss= 6.398,mfm_loss=0.001097748151667294,lr[0]=4.3077224301647115e-05
2022-07-31 17:36:32 - INFO - Epoch=3/5 	 step=64499 	 total_loss= 5.283,itm_loss=0.3085,mlm_loss= 1.775,itc_loss= 6.396,mfm_loss=0.0010792181875933279,lr[0]=4.257288866566391e-05
2022-07-31 17:43:53 - INFO - Epoch=3/5 	 step=64999 	 total_loss= 5.282,itm_loss=0.3083,mlm_loss= 1.775,itc_loss= 6.396,mfm_loss=0.001062750279376287,lr[0]=4.206689788661662e-05
2022-07-31 17:51:13 - INFO - Epoch=3/5 	 step=65499 	 total_loss=  5.28,itm_loss= 0.308,mlm_loss= 1.774,itc_loss= 6.395,mfm_loss=0.0010424806989010084,lr[0]=4.155936255468626e-05
2022-07-31 17:58:33 - INFO - Epoch=3/5 	 step=65999 	 total_loss= 5.279,itm_loss=0.3076,mlm_loss= 1.773,itc_loss= 6.394,mfm_loss=0.0010227585349040275,lr[0]=4.1050393597633884e-05
2022-07-31 18:05:53 - INFO - Epoch=3/5 	 step=66499 	 total_loss= 5.277,itm_loss=0.3074,mlm_loss= 1.772,itc_loss= 6.393,mfm_loss=0.0010103403805913048,lr[0]=4.0540102256556035e-05
2022-07-31 18:13:13 - INFO - Epoch=3/5 	 step=66999 	 total_loss= 5.276,itm_loss=0.3073,mlm_loss= 1.771,itc_loss= 6.392,mfm_loss=0.0010191496622182943,lr[0]=4.00286000615717e-05
2022-07-31 18:20:32 - INFO - Epoch=3/5 	 step=67499 	 total_loss= 5.274,itm_loss= 0.307,mlm_loss=  1.77,itc_loss=  6.39,mfm_loss=0.0010151661479871155,lr[0]=3.951599880744605e-05
2022-07-31 18:27:52 - INFO - Epoch=3/5 	 step=67999 	 total_loss= 5.272,itm_loss=0.3067,mlm_loss=  1.77,itc_loss= 6.389,mfm_loss=0.0010161178132622765,lr[0]=3.900241052915649e-05
2022-07-31 18:35:12 - INFO - Epoch=3/5 	 step=68499 	 total_loss=  5.27,itm_loss=0.3063,mlm_loss= 1.769,itc_loss= 6.387,mfm_loss=0.001003936479435584,lr[0]=3.8487947477406034e-05
2022-07-31 18:42:33 - INFO - Epoch=3/5 	 step=68999 	 total_loss= 5.269,itm_loss=0.3059,mlm_loss= 1.769,itc_loss= 6.386,mfm_loss=0.0009905104853505159,lr[0]=3.7972722094089656e-05
2022-07-31 18:49:53 - INFO - Epoch=3/5 	 step=69499 	 total_loss= 5.267,itm_loss=0.3053,mlm_loss= 1.768,itc_loss= 6.386,mfm_loss=0.0009749595180707616,lr[0]=3.7456846987718726e-05
2022-07-31 18:57:12 - INFO - Epoch=3/5 	 step=69999 	 total_loss= 5.265,itm_loss=0.3051,mlm_loss= 1.767,itc_loss= 6.384,mfm_loss=0.0009606464258734974,lr[0]=3.6940434908809224e-05
2022-07-31 19:04:33 - INFO - Epoch=3/5 	 step=70499 	 total_loss= 5.263,itm_loss=0.3048,mlm_loss= 1.766,itc_loss= 6.383,mfm_loss=0.0009440750502654425,lr[0]=3.6423598725238665e-05
2022-07-31 19:11:52 - INFO - Epoch=3/5 	 step=70999 	 total_loss= 5.262,itm_loss=0.3045,mlm_loss= 1.765,itc_loss= 6.382,mfm_loss=0.0009336702002825948,lr[0]=3.590645139757765e-05
2022-07-31 19:19:12 - INFO - Epoch=3/5 	 step=71499 	 total_loss=  5.26,itm_loss=0.3041,mlm_loss= 1.765,itc_loss= 6.381,mfm_loss=0.0009222860307813542,lr[0]=3.538910595440087e-05
2022-07-31 19:26:32 - INFO - Epoch=3/5 	 step=71999 	 total_loss= 5.259,itm_loss=0.3037,mlm_loss= 1.764,itc_loss=  6.38,mfm_loss=0.0009231339247167919,lr[0]=3.4871675467583564e-05
2022-07-31 19:33:52 - INFO - Epoch=3/5 	 step=72499 	 total_loss= 5.257,itm_loss=0.3035,mlm_loss= 1.764,itc_loss= 6.379,mfm_loss=0.0009093699574901067,lr[0]=3.4354273027588145e-05
2022-07-31 19:41:12 - INFO - Epoch=3/5 	 step=72999 	 total_loss= 5.256,itm_loss=0.3032,mlm_loss= 1.763,itc_loss= 6.378,mfm_loss=0.00089730084651727,lr[0]=3.383701171874717e-05
2022-07-31 19:48:32 - INFO - Epoch=3/5 	 step=73499 	 total_loss= 5.255,itm_loss=0.3028,mlm_loss= 1.763,itc_loss= 6.377,mfm_loss=0.0008840070289661445,lr[0]=3.3320004594547256e-05
2022-07-31 19:55:52 - INFO - Epoch=3/5 	 step=73999 	 total_loss= 5.253,itm_loss=0.3024,mlm_loss= 1.762,itc_loss= 6.376,mfm_loss=0.0008741556564184326,lr[0]=3.280336465292005e-05
2022-07-31 20:03:12 - INFO - Epoch=3/5 	 step=74499 	 total_loss= 5.251,itm_loss=0.3021,mlm_loss= 1.762,itc_loss= 6.374,mfm_loss=0.0008638479245764329,lr[0]=3.2287204811545136e-05
2022-07-31 20:10:32 - INFO - Epoch=3/5 	 step=74999 	 total_loss=  5.25,itm_loss=0.3016,mlm_loss= 1.761,itc_loss= 6.373,mfm_loss=0.0008563043599707569,lr[0]=3.177163788317064e-05
